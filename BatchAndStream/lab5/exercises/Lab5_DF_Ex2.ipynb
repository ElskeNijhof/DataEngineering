{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256a06e8-5c19-493f-b597-b21467b7d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab5_DF_EX2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(\"/home/jovyan/data/online-retail-dataset.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c0260-b2b9-448b-8e5a-0676c4b2364d",
   "metadata": {},
   "source": [
    "What is the average quantity bought by the customer 14769?\n",
    "\n",
    "**collect** function https://sparkbyexamples.com/pyspark/pyspark-collect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d466427d-b243-42dd-9784-c81f350bbb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.769652650822669\n",
      "6.769652650822669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "# 1 \n",
    "print(df.where(df.CustomerID == \"14769\").agg(avg(col(\"Quantity\"))).collect()[0][0])   # first row, first column\n",
    "\n",
    "# 2\n",
    "print(df.where(df.CustomerID == \"14769\").agg({\"Quantity\": \"avg\"}).collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e52541-a2fb-4598-8703-ff9d226de8ec",
   "metadata": {},
   "source": [
    "What is the most occurring word in the items bought by the customers from France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "247e92de-ee6d-4214-9843-2355faa320b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "|   ALARM|\n",
      "|   CLOCK|\n",
      "|BAKELIKE|\n",
      "|    PINK|\n",
      "|   ALARM|\n",
      "|   CLOCK|\n",
      "|BAKELIKE|\n",
      "|     RED|\n",
      "|        |\n",
      "|   ALARM|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|    | 2465|\n",
      "| RED| 1189|\n",
      "| SET| 1128|\n",
      "+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:==================================================>   (188 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, concat, col, lit, desc\n",
    "\n",
    "words = df.where(df.Country == \"France\").select(\"Description\").select(\n",
    "            explode(\n",
    "                split(col(\"Description\"), \" \")\n",
    "            ).alias(\"word\")\n",
    "        )\n",
    "words.show(10)\n",
    "ordered_word_count = words.groupby(words.word).count().orderBy(col(\"count\").desc())\n",
    "ordered_word_count.show(3)\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-collect/\n",
    "print(ordered_word_count.collect()[1][0])   # second row, first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa621131-26cb-48a8-8f6b-6e01934395ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
